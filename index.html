<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="profile.jpg" alt="alt text" width="140px" height="HEIGHTpx" />&nbsp;</td>
<td align="left"><h2>Kumar Kshitij Patel<br /></h2>
  <p>PhD student,<br />
    <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago (TTIC) </a><br />
    6045 S. Kenwood Ave., Chicago, IL<br />
    email: <a href="mailto:kkpatel@ttic.edu">kkpatel@ttic.edu</a></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a fourth year PhD student at Toyota Technological Institute at Chicago (TTIC) 
  advised by <a href="https://home.ttic.edu/~nati/">Prof. Nati Srebro</a> and <a href="https://lingxiaowang-ai.github.io/">Prof. Lingxiao Wang</a>. I am interested in understanding optimization algorithms 
  in practically relevant settings, such as settings with distributed computation and non-convexity. I am interested in understanding 
  the min-max oracle complexity of optimization for these settings specially 
  in presence of data and systems heterogeneity. Recently, I have been studying the 
  game-theoretic considerations for collaboration protocols, such as cross-device federated learning. And also exploring some online robust combinatorial problems with <a href="https://faculty.chicagobooth.edu/rad-niazadeh">Prof. Rad Niazadeh</a> at the Chicago 
  Booth School. 

<p>During summer 2020, I worked with the amazing team at <a href="https://aws.amazon.com/codeguru/">Codeguru, Amazon Web Services</a> as an applied scientist intern. And before joining TTIC, I obtained my BTech in Computer Science and Engineering at 
  <a href="https://www.iitk.ac.in/">Indian Institute of Technology, Kanpur</a>. There I was fortunate to work with 
  <a href="https://www.cse.iitk.ac.in/users/purushot/">Prof. Purushottam Kar</a> on Bandit Learning algorithms. I also spent a year of 
  my undergraduate on academic exchange at École Polytechnique Fédérale de Lausanne (EPFL) 
  where I worked at the <a href="https://www.epfl.ch/labs/mlo/">Machine Learning and Optimization Laboratory (MLO)</a> 
  with <a href="https://people.epfl.ch/martin.jaggi">Prof. Martin Jaggi</a>.</p>

<a href="CV.pdf">[Curriculum Vitae]</a> <a href="https://scholar.google.com/citations?hl=en&user=Okd0qN0AAAAJ&scilu=&scisig=AMD79ooAAAAAYWxekJtWOS5ROvCHMf5IQ8qk0xQ0A06B&gmla=AJsN-F6eMAtkmL_kwGrUvC7-JJn6Ucd64fOlVNAnT2mpa1uJU-suhrtCD7E3S7rXptEcsYrTA4KMiM2VyHR2dENvSU9LuMnkpCG_1drE3pMjk51EZpqTR_M&sciund=11895351974128197353">[Google Scholar]</a> </br>
</br>
<b><em>I am co-organizing a <a href="https://sites.google.com/view/tticfl-summerworkshop2023/home">workshop on theoritical advances in Federated Learning</a> this summer at TTIC.</em></b>
</br>
<b><em>I am currently looking for research internships for 2024.</em></b>

<h2>Publications</h2>
<ul>
  <li><p><a href="CMC.pdf"><b>On Convexity and Linear Mode Connectivity in Neural Networks</b></a> <br /> David Yunis, <em>Kumar Kshitij Patel</em>, Pedro Savarese, Karen Livescu, Matthew Walter, Jonathan Frankle, Michael Maire<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="FEDOSGD.pdf"><b>Distributed Online and Bandit Convex Optimization</b></a> <br /> <em>Kumar Kshitij Patel</em>, Aadrirupa Saha, Lingxiao Wang, Nathan Srebro<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="CELSGD.pdf"><b>Towards Optimal communication complexity in Distributed Non-convex Optimization</b></a> <a href="https://qrco.de/bdWwG5">[Recorded Talk]</a><br /> <em>Kumar Kshitij Patel</em>*, Lingxiao Wang*, Blake Woodworth, Brian Bullins, Nathan Srebro (*Equal Contribution)<br /> NeurIPS 2022</p>
  </li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.02954"><b>A Stochastic Newton Algorithm for Distributed Convex Optimization</b></a> <a href="https://www.youtube.com/watch?v=-AS5NWOaNqk">[Recorded Talk]</a><br /> Brian Bullins, <em>Kumar Kshitij Patel</em>, Ohad Shamir, Nathan Srebro, Blake Woodworth (Alphabetical ordering)<br /> NeurIPS 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.04735"><b>Minibatch vs Local SGD for Heterogeneous Distributed Learning</b></a> <a href="https://videos.neurips.cc/search/minibatch%20vs/video/slideslive-38937666">[Recorded Talk]</a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Nathan Srebro <br /> NeurIPS 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.07839"><b>Is Local SGD Better than Minibatch SGD?</b></a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nathan Srebro <br /> ICML 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1808.07217#:~:text=Mini%2Dbatch%20stochastic%20gradient%20methods,scalability%20gains%20in%20recent%20years."><b>Don't Use Large Mini-batches, Use Local SGD</b></a><a href="https://github.com/epfml/LocalSGD-Code"> [Code]</a> <br /> Tao Lin, Sebastian U. Stich, <em>Kumar Kshitij Patel</em>, Martin Jaggi <br /> ICLR 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf"><b>Communication Trade-offs for Local-SGD with Large Step Size</b></a> <br /> <em>Kumar Kshitij Patel, Aymeric Dieuleveut</em> <br /> NeurIPS 2019</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5758-5"><b>Corruption-Tolerant Bandit Learning</b></a> <br /> Sayash Kapoor, <em>Kumar Kshitij Patel, Purushottam Kar</em> <br /> Springer Machine Learning Journal 2019</p>
</li>
</ul>

<h2>Other Research Activities</h2>
<ul>
<li><p>I served/am serving as a reviewer for STOC'21, TMLR, ICML'21'22, NeurIPS'21'22, ICLR'22'23, AISTATS'22'23, Springer MLJ, as a session chair for ICML'22, NeurIPS'22, and as a volunteer for ICML'20, ICLR'20. I received the top reviewer award at ICLR'22, ICML'22, NeurIPS'22.</p>
</li>
<li><p>I am participating in the NSF-Simon's research collaboration on the <a href="https://www.simonsfoundation.org/grant/nsf-simons-research-collaborations-on-the-mathematical-and-scientific-foundations-of-deep-learning/">Mathematics of Deep Learning (MoDL)</a>.</p>
</li>
<li><p>I co-organized the <a href="https://ttic-student-workshop.github.io/">TTIC Student Workshop 2021</a>, with <a href="https://gxli97.github.io/">Gene Li</a>. We also organized a TTIC/Uchicago student theory seminar in Spring 2021. If you'd like to take over and re-start this series, please let me know.</p>
</li>
<li><p>I was a Teaching Assistant for the Convex Optmization course at TTIC and a co-organizer for the <a href="https://www.ttic.edu/research-at-ttic/">Research at TTIC Colloquium</a> for Fall-Winter 2021.</p>
</li>
<li><p>I participated in the <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School at T&uumlbingen, Germany</a> during summer 2020.</p></li>
</ul>

</div>
</body>
</html>

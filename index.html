<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="profile.jpg" alt="alt text" width="140px" height="HEIGHTpx" />&nbsp;</td>
<td align="left"><h2>Kumar Kshitij Patel<br /></h2>
  <p>PhD student,<br />
    <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago (TTIC) </a><br />
    6045 S. Kenwood Ave., Chicago, IL<br />
    email: <a href="mailto:kkpatel@ttic.edu">kkpatel@ttic.edu</a></p>
</td></tr></table>
<h2>About Me</h2>
<p>I am a third year PhD student at Toyota Technological Institute at Chicago (TTIC) 
  advised by <a href="https://home.ttic.edu/~nati/">Prof. Nati Srebro</a>. I am interested in understanding optimization algorithms 
  in practically relevant settings, i.e., with distributed computation and non-convexity. This 
  has motivated me to study the oracle complexity of optimization for Federated Learning, specially 
  in settings with data and systems heterogeneity. Recently, I have been trying to understand the 
  game-theoretic considerations for collaboration protocols with <a href="https://faculty.chicagobooth.edu/rad-niazadeh">Prof. Rad Niazadeh</a> at the Chicago 
  Booth School. I am also the current organizer for the <em>Machine Learning and Optimization Reading Group</em> at TTIC. 
  If you have a work you'd like to present at our reading group, or want us to know about, please ping <a href="mailto:nati@ttic.edu">Nati</a> or me.</p>

<p>During summer 2020, I worked with the amazing team at <a href="https://aws.amazon.com/codeguru/">Codeguru, Amazon Web Service</a> as an applied scientist intern. And before joining TTIC, I obtained my BTech in Computer Science and Engineering at 
  <a href="https://www.iitk.ac.in/">Indian Institute of Technology, Kanpur</a>. There I was fortunate to work with 
  <a href="https://www.cse.iitk.ac.in/users/purushot/">Prof. Purushottam Kar</a> on Bandit Learning algorithms. I also spent a year of 
  my undergraduate on academic exchange at École Polytechnique Fédérale de Lausanne (EPFL) 
  where I worked at the <a href="https://www.epfl.ch/labs/mlo/">Machine Learning and Optimization Laboratory (MLO)</a> 
  with <a href="https://people.epfl.ch/martin.jaggi">Prof. Martin Jaggi</a>.</p>

<a href="CV.pdf">[Curriculum Vitae]</a> <a href="https://scholar.google.com/citations?hl=en&user=Okd0qN0AAAAJ&scilu=&scisig=AMD79ooAAAAAYWxekJtWOS5ROvCHMf5IQ8qk0xQ0A06B&gmla=AJsN-F6eMAtkmL_kwGrUvC7-JJn6Ucd64fOlVNAnT2mpa1uJU-suhrtCD7E3S7rXptEcsYrTA4KMiM2VyHR2dENvSU9LuMnkpCG_1drE3pMjk51EZpqTR_M&sciund=11895351974128197353">[Google Scholar]</a> </br>
</br>
<b><em>I am currently looking for research internships for 2022.</em></b>

<h2>Publications</h2>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.02954"><b>A Stochastic Newton Algorithm for Distributed Convex Optimization</b></a> <a href="https://www.youtube.com/watch?v=-AS5NWOaNqk">[Recorded Talk]</a><br /> Brian Bullins, <em>Kumar Kshitij Patel</em>, Ohad Shamir, Nathan Srebro, Blake Woodworth (Alphabetical ordering)<br /> NeurIPS 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.04735"><b>Minibatch vs Local SGD for Heterogeneous Distributed Learning</b></a> <a href="https://videos.neurips.cc/search/minibatch%20vs/video/slideslive-38937666">[Recorded Talk]</a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Nathan Srebro <br /> NeurIPS 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.07839"><b>Is Local SGD Better than Minibatch SGD?</b></a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nathan Srebro <br /> ICML 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1808.07217#:~:text=Mini%2Dbatch%20stochastic%20gradient%20methods,scalability%20gains%20in%20recent%20years."><b>Don't Use Large Mini-batches, Use Local SGD</b></a><a href="https://github.com/epfml/LocalSGD-Code"> [Code]</a> <br /> Tao Lin, Sebastian U. Stich, <em>Kumar Kshitij Patel</em>, Martin Jaggi <br /> ICLR 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf"><b>Communication Trade-offs for Local-SGD with Large Step Size</b></a> <br /> <em>Kumar Kshitij Patel, Aymeric Dieuleveut</em> <br /> NeurIPS 2019</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5758-5"><b>Corruption-Tolerant Bandit Learning</b></a> <br /> Sayash Kapoor, <em>Kumar Kshitij Patel, Purushottam Kar</em> <br /> Springer Machine Learning Journal 2019</p>
</li>
</ul>

<h2>Other Research Activities</h2>
<ul>
<li><p>I served/am serving as a reviewer for STOC'21, ICML'21, NeurIPS'21, ICLR'22, AISTATS'22, Springer MLJ, and as a volunteer for ICML'20, ICLR'20.</p>
</li>
<li><p>I am participating in the NSF-Simon's research collaboration on the <a href="https://www.simonsfoundation.org/grant/nsf-simons-research-collaborations-on-the-mathematical-and-scientific-foundations-of-deep-learning/">Mathematics of Deep Learning (MoDL)</a>.</p>
</li>
<li><p>I co-organized the <a href="https://ttic-student-workshop.github.io/">TTIC Student Workshop 2021</a>, with <a href="https://gxli97.github.io/">Gene Li</a>. We also organized a TTIC/Uchicago student theory seminar in Spring 2021. If you'd like to take over and re-start this series, please let me know.</p>
</li>
<li><p>I was also a Teaching Assistant and co-organizer for the <a href="https://www.ttic.edu/research-at-ttic/">Research at TTIC Colloquium</a> for Fall-Winter 2021.</p>
</li>
<li><p>I took part in the <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School at T&uumlbingen, Germany</a> during summer 2020.</p></li>
</ul>

</div>
</body>
</html>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Kumar Kshitij Patel</title>
<style>
  body {
    background-color: #e6f7ff; /* Very light shade of blue */
    font-family: Arial, sans-serif;
    color: #333;
    margin: 0;
    padding: 0;
  }
  .top-right-buttons {
    position: fixed;
    top: 20px;
    right: 20px;
    display: flex;
    flex-wrap: wrap;
    gap: 10px;
  }
  .top-right-buttons a {
    display: inline-block;
    padding: 10px 20px;
    background-color: #0073e6;
    color: white;
    text-decoration: none;
    border-radius: 5px;
  }
  .top-right-buttons a:hover {
    background-color: #005bb5;
  }
  #layout-content {
    max-width: 1000px;
    margin: 80px auto;
    padding: 20px;
    background-color: #ffffff; /* White background for the content area */
    border-radius: 10px;
    box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
  }
  h1, h2 {
    color: #0073e6;
  }
  a {
    color: #0073e6;
  }
  a:hover {
    color: #005bb5;
  }
  @media (max-width: 600px) {
    .top-right-buttons {
      position: static;
      justify-content: center;
      margin-top: 20px;
    }
    #layout-content {
      margin: 20px;
    }
  }
</style>
</head>
<body>
<div class="top-right-buttons">
  <a href="CV.pdf" target="_blank">CV</a>
  <a href="https://scholar.google.com/citations?hl=en&user=Okd0qN0AAAAJ" target="_blank">Google Scholar</a>
  <a href="research.html">Projects</a>
  <a href="interests.html">Research Interests</a>
</div>

<div id="layout-content">
<table class="imgtable"><tr><td>
<img src="profile_3.jpg" alt="alt text" width="250px" height="HEIGHTpx" />&nbsp;</td>
</br><td align="left"><h1>Kumar Kshitij Patel<br /></h1>
  <p>PhD student,<br />
    <a href="https://www.ttic.edu/">Toyota Technological Institute at Chicago (TTIC) </a><br />
    6045 S. Kenwood Ave., Chicago, IL<br />
    email: <a href="mailto:kkpatel@ttic.edu">kkpatel@ttic.edu</a></p>
</td></tr></table>
<h2>About Me</h2>

<p>
I am a fifth-year PhD student at the Toyota Technological Institute at Chicago (TTIC), where I have the privilege of 
being advised by <a href="https://home.ttic.edu/~nati/">Prof. Nati Srebro</a> and 
<a href="https://lingxiaowang-ai.github.io/">Prof. Lingxiao Wang</a>. 
Throughout my PhD, I have explored various facets of collaborative learning, 
focusing on proving theoretical guarantees for optimization and ensuring the 
privacy of distributed algorithms amid data and systems heterogeneity. Recently, 
my research has expanded to examine the incentives that encourage agents to initiate and sustain these collaborations.  
A more in depth discussion about my research projects can be found <a href="research.html"><b>here</b></a>.
</p>

<p>
For an (mostly) up-to-date list of my publications, please visit my <a href="https://scholar.google.com/citations?hl=en&user=Okd0qN0AAAAJ"><b>Google Scholar</b></a> profile. 
You can also access my <a href="CV.pdf" target="_blank"><b>CV here</b></a>. 
</p>

<p>During Summer 2023, I worked with <a href="https://ngazagna.github.io/">Nidham Gazagnadou</a> and <a href="https://sites.google.com/view/lingjuan-lyu/home">Lingjuan Lyu</a> from the Privacy Preserving Machine Learning team at <a href="https://ai.sony/">Sony AI</a> in Tokyo, Japan as a research intern. During summer 2020, I worked with the amazing team at <a href="https://aws.amazon.com/codeguru/">Codeguru, Amazon Web Services</a> as an applied scientist intern. And before joining TTIC, I obtained my BTech in Computer Science and Engineering at 
  <a href="https://www.iitk.ac.in/">Indian Institute of Technology, Kanpur</a>. There I was fortunate to work with 
  <a href="https://www.cse.iitk.ac.in/users/purushot/">Prof. Purushottam Kar</a> on Bandit Learning algorithms. I also spent a year of 
  my undergraduate on academic exchange at École Polytechnique Fédérale de Lausanne (EPFL) 
  where I worked at the <a href="https://www.epfl.ch/labs/mlo/">Machine Learning and Optimization Laboratory (MLO)</a> 
  with <a href="https://people.epfl.ch/martin.jaggi">Prof. Martin Jaggi</a>.</p>

<p><b><em>I am actively looking for postdoctoral positions starting next year in Fall 2025.</em></b> I am interested in exploring research areas both related to and separate from my current projects, as detailed in my evolving <a href="interests.html"><b>research interests</b></a>.</p>


<!-- <h2>Publications</h2>
<ul>
  <li><p><a href="CMC.pdf"><b>On Convexity and Linear Mode Connectivity in Neural Networks</b></a> <br /> David Yunis, <em>Kumar Kshitij Patel</em>, Pedro Savarese, Karen Livescu, Matthew Walter, Jonathan Frankle, Michael Maire<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="FEDOSGD.pdf"><b>Distributed Online and Bandit Convex Optimization</b></a> <br /> <em>Kumar Kshitij Patel</em>, Aadrirupa Saha, Lingxiao Wang, Nathan Srebro<br />OPT ML Workshop, NeurIPS 2022</p>
  </li>
</ul>
<ul>
  <li><p><a href="CELSGD.pdf"><b>Towards Optimal communication complexity in Distributed Non-convex Optimization</b></a> <a href="https://qrco.de/bdWwG5">[Recorded Talk]</a><br /> <em>Kumar Kshitij Patel</em>*, Lingxiao Wang*, Blake Woodworth, Brian Bullins, Nathan Srebro (*Equal Contribution)<br /> NeurIPS 2022</p>
  </li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2110.02954"><b>A Stochastic Newton Algorithm for Distributed Convex Optimization</b></a> <a href="https://www.youtube.com/watch?v=-AS5NWOaNqk">[Recorded Talk]</a><br /> Brian Bullins, <em>Kumar Kshitij Patel</em>, Ohad Shamir, Nathan Srebro, Blake Woodworth (Alphabetical ordering)<br /> NeurIPS 2021</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2006.04735"><b>Minibatch vs Local SGD for Heterogeneous Distributed Learning</b></a> <a href="https://videos.neurips.cc/search/minibatch%20vs/video/slideslive-38937666">[Recorded Talk]</a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Nathan Srebro <br /> NeurIPS 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/2002.07839"><b>Is Local SGD Better than Minibatch SGD?</b></a> <br /> Blake Woodworth, <em>Kumar Kshitij Patel</em>, Sebastian U. Stich, Zhen Dai, Brian Bullins, H. Brendan McMahan, Ohad Shamir, Nathan Srebro <br /> ICML 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://arxiv.org/abs/1808.07217#:~:text=Mini%2Dbatch%20stochastic%20gradient%20methods,scalability%20gains%20in%20recent%20years."><b>Don't Use Large Mini-batches, Use Local SGD</b></a><a href="https://github.com/epfml/LocalSGD-Code"> [Code]</a> <br /> Tao Lin, Sebastian U. Stich, <em>Kumar Kshitij Patel</em>, Martin Jaggi <br /> ICLR 2020</p>
</li>
</ul>
<ul>
<li><p><a href="https://proceedings.neurips.cc/paper/2019/file/4aadd661908b181d059a117f02fbc9ec-Paper.pdf"><b>Communication Trade-offs for Local-SGD with Large Step Size</b></a> <br /> <em>Kumar Kshitij Patel, Aymeric Dieuleveut</em> <br /> NeurIPS 2019</p>
</li>
</ul>
<ul>
<li><p><a href="https://link.springer.com/article/10.1007/s10994-018-5758-5"><b>Corruption-Tolerant Bandit Learning</b></a> <br /> Sayash Kapoor, <em>Kumar Kshitij Patel, Purushottam Kar</em> <br /> Springer Machine Learning Journal 2019</p>
</li>
</ul> -->

<h2>News</h2>
<ul>
  <li><p>I am attending <a href="https://learningtheory.org/colt2024/">COLT</a> to present <a href="https://arxiv.org/abs/2405.11667">our paper</a> on lower and upper bounds for local SGD. Come say hi!</p></li>
  <li><p>In July, I am visiting <a href="https://www.sstich.ch/">Sebastian Stich</a>'s group at the <a href="https://cispa.de/en">CISPA Helmhotz Center</a>.</p></li>
  <li><p>I am attending <a href="https://icml.cc/">ICML</a> to present some workshop papers, and organize the Data Economics social along with <a href="https://www.katedonahue.me/">Kate Donahue</a>. Please come to our social, if you are interested in or want to know more about the economics of data!</p></li>
  <li><p>In August, I am attending <a href="https://ijcai24.org/">IJCAI</a> to present our papers on <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4824251">online optimization with fairness</a> and <a href="https://arxiv.org/pdf/2311.16459#:~:text=Defections%2C%20or%20the%20act%20of,information%20for%20the%20learning%20process.">defections in Federated Learning</a>.</p></li>
</ul>

<h2>Other Research Activities</h2>
<ul>

<li><p>I organized a workshop on <a href="https://sites.google.com/view/tticfl-summerworkshop2023/home">Theoritical Advances in Federated Learning</a> last summer (2023) at TTIC.</p></li>  
<li><p>I co-taught a tutorial at <a href="https://www.auai.org/uai2023/">UAI'23</a> titled <a href="https://www.auai.org/uai2023/tutorials">Online Optimization meets Federated Learning</a>.</p></li>
<li><p>I served/am serving as a reviewer for STOC'21, TMLR, JMLR, ICML'21'22'24, NeurIPS'21'22'23'24, ICLR'22'23'24, AISTATS'22'23, Springer MLJ, as a session chair for ICML'22, NeurIPS'22, and as a volunteer for IJCAI'24, ICML'20, ICLR'20. I received the top reviewer award at ICLR'22, ICML'22, NeurIPS'22.</p>
</li>
<li><p>I am participating in the NSF-Simon's research collaboration on the <a href="https://www.simonsfoundation.org/grant/nsf-simons-research-collaborations-on-the-mathematical-and-scientific-foundations-of-deep-learning/">Mathematics of Deep Learning (MoDL)</a>.</p>
</li>
<li><p>I co-organized the <a href="https://ttic-student-workshop.github.io/">TTIC Student Workshop 2021</a>, with <a href="https://gxli97.github.io/">Gene Li</a>. We also organized a TTIC/Uchicago student theory seminar in Spring 2021. If you'd like to take over and re-start this series, please let me know.</p>
</li>
<li><p>I was a Teaching Assistant for the Convex Optmization course at TTIC during Winter'22'24 and a co-organizer for the <a href="https://www.ttic.edu/research-at-ttic/">Research at TTIC Colloquium</a> for Fall-Winter 2021.</p>
</li>
<li><p>I participated in the <a href="http://mlss.tuebingen.mpg.de/2020/">Machine Learning Summer School at T&uumlbingen, Germany</a> during summer 2020.</p></li>
</ul>

</div>
</body>
</html>
